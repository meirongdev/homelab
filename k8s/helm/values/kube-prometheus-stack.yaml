# Prometheus + Grafana + Alertmanager
grafana:
  enabled: true
  admin:
    existingSecret: "grafana-admin-credentials" # ✅ 使用预先创建的 Secret
    userKey: ""              # ✅ 显式设置为空，覆盖默认值
    passwordKey: "admin-password" 

  # 持久化存储
  persistence:
    enabled: true
    storageClassName: nfs-client
    size: 1Gi

  # 资源限制
  resources:
    requests:
      cpu: 100m                 # ✅ 从默认值降低
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
  
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki-gateway.monitoring.svc.cluster.local
      access: proxy
      isDefault: false
      jsonData:
        derivedFields:
          - name: TraceID
            matcherRegex: "traceID=(\\w+)"
            url: "$${__value.raw}"
            datasourceUid: tempo
    - name: Tempo
      type: tempo
      url: http://tempo.monitoring.svc.cluster.local:3100
      access: proxy
      isDefault: false
      jsonData:
        tracesToLogs:
          datasourceUid: loki
          lokiSearch: true

prometheus:
  prometheusSpec:
    retention: 7d
    externalLabels:
      cluster: homelab
    
    # 持久化存储
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: nfs-client
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi
    
    # 资源限制
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 1Gi


    # 添加额外的抓取配置
    additionalScrapeConfigs:
      - job_name: 'node-exporter-metal-nodes'
        scrape_interval: 15s
        static_configs:
          - targets:
              - 192.168.50.106:9100  # storage-node-1
              - 192.168.50.4:9100    # proxmox-node-1
              # - 10.10.10.10:9100   # k8s-node

      # oracle-k3s 指标均为 push 模式，不需要在此 scrape：
      #   node-exporter / kube-state-metrics / postgres-exporter
      #     → prometheus-agent (oracle-k3s) remote_write → :31090/api/v1/write
      #   cloudflared / traefik
      #     → OTel Collector (oracle-k3s) prometheusremotewrite → :31090/api/v1/write
      # See: cloud/oracle/manifests/monitoring/prometheus-agent-values.yaml
      #      cloud/oracle/manifests/monitoring/otel-collector.yaml

    # Enable OTLP receiver so OTel Collectors can push metrics directly
    enableFeatures:
      - otlp-write-receiver

    # Web endpoints:
    #   /api/v1/otlp  — homelab OTel Collector pushes metrics (OTLP)
    #   /api/v1/write — oracle prometheus-agent + oracle OTel prometheusremotewrite
    additionalArgs:
      - name: web.enable-otlp-receiver
      - name: web.enable-remote-write-receiver

alertmanager:
  enabled: true
  alertmanagerSpec:
    resources:
      requests:
        cpu: 50m             
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: nfs-client
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi

# 默认会安装很多 ServiceMonitors，如果资源有限可以禁用一些
kubeStateMetrics:
  enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 128Mi
nodeExporter:
  enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 128Mi

prometheusOperator:
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi

# 如果不需要监控 etcd、scheduler 等控制平面组件，可以禁用
kubeEtcd:
  enabled: false

kubeScheduler:
  enabled: false

kubeControllerManager:
  enabled: false