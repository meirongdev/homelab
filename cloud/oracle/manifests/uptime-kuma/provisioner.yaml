---
# Provisioner script — creates admin account + all monitors on first deploy
# Run manually: just provision-uptime-kuma
apiVersion: v1
kind: ConfigMap
metadata:
  name: uptime-kuma-provisioner
  namespace: personal-services
data:
  provision.py: |
    #!/usr/bin/env python3
    import os, sys, time, urllib.request

    UPTIME_KUMA_URL = os.environ.get("UPTIME_KUMA_URL", "http://uptime-kuma:3001")
    USERNAME = os.environ.get("ADMIN_USERNAME", "admin")
    PASSWORD = os.environ["ADMIN_PASSWORD"]

    # All internal URLs bypass SSO entirely.
    # oracle-k3s services: direct ClusterIP via local CNI.
    # k3s-homelab services: direct ClusterIP via Tailscale subnet routing
    #   (oracle node advertises 10.52/16+10.53/16; homelab node advertises 10.42/16+10.43/16).
    #   Requires iptables MASQUERADE on oracle node for pod→10.43.0.0/16 traffic
    #   and ip route for 10.42-43.0.0/16 added to pod network namespace.
    MONITORS = [
        # --- oracle-k3s local services ---
        {"name": "Homepage",       "url": "http://homepage.homepage.svc:3000"},
        {"name": "Miniflux",       "url": "http://miniflux.rss-system.svc:8080"},
        {"name": "IT-Tools",       "url": "http://it-tools.personal-services.svc:80"},
        {"name": "Squoosh",        "url": "http://squoosh.personal-services.svc:8080"},
        {"name": "Stirling-PDF",   "url": "http://stirling-pdf.personal-services.svc:8080/login"},
        {"name": "Uptime Kuma",    "url": "http://uptime-kuma.personal-services.svc:3001"},
        # --- k3s-homelab services via Tailscale (ClusterIP 10.43.x.x) ---
        # Grafana: /api/health returns 200 without auth
        {"name": "Grafana",        "url": "http://10.43.70.241:80/api/health"},
        # Vault: /v1/sys/health returns 200 when initialized and unsealed
        {"name": "HashiCorp Vault","url": "http://10.43.29.201:8200/v1/sys/health"},
        # Calibre-Web: /login returns 200 without auth
        {"name": "Calibre-Web",    "url": "http://10.43.206.163:8083/login"},
        # ArgoCD: HTTP→307→HTTPS; use HTTPS directly (self-signed cert)
        {"name": "ArgoCD",         "url": "https://10.43.158.192/",      "ignoreTls": True},
        # Kopia: HTTPS with self-signed cert, returns 401 (basic auth) — service is up
        {"name": "Kopia Backup",   "url": "https://10.43.114.55:51515",  "accepted_statuscodes": ["401"], "ignoreTls": True},
        # --- new info pipeline services ---
        {"name": "KaraKeep",       "url": "http://karakeep.rss-system.svc:3000", "accepted_statuscodes": ["307"]},
        {"name": "Gotify",         "url": "http://10.43.223.13:80/health"},
    ]

    def wait_ready(timeout=120):
        deadline = time.time() + timeout
        while time.time() < deadline:
            try:
                urllib.request.urlopen(UPTIME_KUMA_URL, timeout=5)
                print("Uptime Kuma is ready.")
                return
            except Exception:
                print("Waiting for Uptime Kuma...")
                time.sleep(5)
        print("ERROR: Uptime Kuma not ready after timeout", file=sys.stderr)
        sys.exit(1)

    wait_ready()

    from uptime_kuma_api import UptimeKumaApi, MonitorType
    api = UptimeKumaApi(UPTIME_KUMA_URL)

    try:
        api.setup(username=USERNAME, password=PASSWORD)
        print("Admin account created.")
    except Exception as e:
        print(f"Setup failed (likely already exists), logging in: {e}")
        api.login(username=USERNAME, password=PASSWORD)
        print("Logged in.")

    STATUS_PAGE_SLUG   = "homelab"
    STATUS_PAGE_TITLE  = "Homelab Status"
    STATUS_PAGE_DOMAIN = "status.meirong.dev"

    # --- Monitors ---
    existing = {m["name"]: m for m in api.get_monitors()}
    print(f"Existing monitors: {set(existing.keys()) or '(none)'}")

    for m in MONITORS:
        extra = {}
        if "accepted_statuscodes" in m:
            extra["accepted_statuscodes"] = m["accepted_statuscodes"]
        else:
            extra["accepted_statuscodes"] = ["200-299"]
        if "ignoreTls" in m:
            extra["ignoreTls"] = m["ignoreTls"]
        if m["name"] in existing:
            # Always update URL and accepted_statuscodes to ensure they stay in sync
            api.edit_monitor(existing[m["name"]]["id"], url=m["url"], **extra)
            print(f"  updated {m['name']} -> {m['url']} (accepted={extra['accepted_statuscodes']})")
            continue
        api.add_monitor(type=MonitorType.HTTP, name=m["name"], url=m["url"], interval=60, **extra)
        print(f"  added {m['name']} -> {m['url']}")

    # --- Public status page ---
    monitor_entries  = [{"id": m["id"], "sendUrl": False} for m in api.get_monitors()]
    existing_pages   = {p["slug"] for p in api.get_status_pages()}

    if STATUS_PAGE_SLUG not in existing_pages:
        api.add_status_page(slug=STATUS_PAGE_SLUG, title=STATUS_PAGE_TITLE)
        print(f"Status page '{STATUS_PAGE_SLUG}' created.")

    api.save_status_page(
        slug=STATUS_PAGE_SLUG,
        title=STATUS_PAGE_TITLE,
        description="Real-time status of Homelab services",
        theme="auto",
        published=True,
        showTags=False,
        domainNameList=[STATUS_PAGE_DOMAIN],
        customCSS="",
        footerText="",
        showPoweredBy=False,
        publicGroupList=[{
            "name": "Services",
            "weight": 1,
            "monitorList": monitor_entries,
        }],
    )
    print(f"Status page updated: https://{STATUS_PAGE_DOMAIN}")

    api.disconnect()
    print("Provisioning complete.")
---
# Standalone Job (not ArgoCD hook — oracle-k3s has no ArgoCD)
# Run manually: just provision-uptime-kuma
apiVersion: batch/v1
kind: Job
metadata:
  name: uptime-kuma-provisioner
  namespace: personal-services
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: provisioner
          image: python:3.12-slim
          command: ["/bin/sh", "-c"]
          args:
            - pip install --quiet uptime-kuma-api==1.2.1 && python /scripts/provision.py
          env:
            - name: UPTIME_KUMA_URL
              value: "http://uptime-kuma.personal-services.svc:3001"
            - name: ADMIN_USERNAME
              valueFrom:
                secretKeyRef:
                  name: uptime-kuma-admin
                  key: username
            - name: ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: uptime-kuma-admin
                  key: password
          volumeMounts:
            - name: scripts
              mountPath: /scripts
      volumes:
        - name: scripts
          configMap:
            name: uptime-kuma-provisioner
